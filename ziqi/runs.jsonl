{"ts": "2025-08-10T22:03:41.420115+00:00", "repo": "pytorch/pytorch", "issue_number": 160273, "issue_url": "https://github.com/pytorch/pytorch/issues/160273", "title": "[Documentation Clarity] torch.min/torch.max gradient behavior", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:03:51.021895+00:00", "repo": "pytorch/pytorch", "issue_number": 160272, "issue_url": "https://github.com/pytorch/pytorch/issues/160272", "title": "Add Green Context Support for fsdp2", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:04:06.963139+00:00", "repo": "pytorch/pytorch", "issue_number": 160271, "issue_url": "https://github.com/pytorch/pytorch/issues/160271", "title": "Error when using torch.compile with dynamic=True: tensor 'guidance' stride mismatch at index 0", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:04:29.648265+00:00", "repo": "pytorch/pytorch", "issue_number": 160269, "issue_url": "https://github.com/pytorch/pytorch/issues/160269", "title": "Questions about Volta Support", "fixed": true, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:04:38.466207+00:00", "repo": "pytorch/pytorch", "issue_number": 160265, "issue_url": "https://github.com/pytorch/pytorch/issues/160265", "title": "XPU out of memory on Intel iGPUs although plenty of memory is available per error message.", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:12:07.958118+00:00", "repo": "pytorch/pytorch", "issue_number": 160281, "issue_url": "https://github.com/pytorch/pytorch/issues/160281", "title": "Improve CUDAGraph Tree heuristics on starting new generation", "labels": ["triaged", "module: cuda graphs", "oncall: pt2", "module: inductor"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:12:23.353541+00:00", "repo": "pytorch/pytorch", "issue_number": 160273, "issue_url": "https://github.com/pytorch/pytorch/issues/160273", "title": "[Documentation Clarity] torch.min/torch.max gradient behavior", "labels": ["module: docs", "triaged", "actionable", "module: python frontend"], "type": "docs", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:12:30.640644+00:00", "repo": "pytorch/pytorch", "issue_number": 160272, "issue_url": "https://github.com/pytorch/pytorch/issues/160272", "title": "Add Green Context Support for fsdp2", "labels": ["oncall: distributed"], "type": "feature", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:12:37.732064+00:00", "repo": "pytorch/pytorch", "issue_number": 160271, "issue_url": "https://github.com/pytorch/pytorch/issues/160271", "title": "Error when using torch.compile with dynamic=True: tensor 'guidance' stride mismatch at index 0", "labels": ["oncall: pt2"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:12:54.321247+00:00", "repo": "pytorch/pytorch", "issue_number": 160269, "issue_url": "https://github.com/pytorch/pytorch/issues/160269", "title": "Questions about Volta Support", "labels": ["module: docs", "oncall: releng", "triaged"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:13:02.641693+00:00", "repo": "pytorch/pytorch", "issue_number": 160265, "issue_url": "https://github.com/pytorch/pytorch/issues/160265", "title": "XPU out of memory on Intel iGPUs although plenty of memory is available per error message.", "labels": ["triaged", "module: xpu"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "[MODEL ERROR] ValueError: Model Qwen/Qwen2.5-Coder-1.5B-Instruct is not supported for task text-generation and provider featherless-ai. Supported task: conversational."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] StopIteration: "}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] StopIteration: "}]}
{"ts": "2025-08-10T22:30:47.672824+00:00", "repo": "pytorch/pytorch", "issue_number": 160281, "issue_url": "https://github.com/pytorch/pytorch/issues/160281", "title": "Improve CUDAGraph Tree heuristics on starting new generation", "labels": ["triaged", "module: cuda graphs", "oncall: pt2", "module: inductor"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "Category: Performance Optimization\n\nNext Steps:\n- Investigate the specific cause of the error by examining the stack trace.\n- Ensure that the `torch.compile` function is correctly used with the appropriate mode (`\"reduce-overhead\"`).\n- Verify that the input data types and shapes are compatible with the compiled functions.\n- Consider using different modes or configurations for `torch.compile` to see if they improve performance.\n- Check for any potential conflicts or issues with other parts of your codebase that might be affecting the compilation process.\n\nReferences:\n- [PyTorch Documentation - torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)\n- [PyTorch Documentation - CUDAGraph Trees](https://pytorch.org/docs/stable/autograd/cudagraph-trees.html)"}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}]}
{"ts": "2025-08-10T22:31:15.128603+00:00", "repo": "pytorch/pytorch", "issue_number": 160273, "issue_url": "https://github.com/pytorch/pytorch/issues/160273", "title": "[Documentation Clarity] torch.min/torch.max gradient behavior", "labels": ["module: docs", "triaged", "actionable", "module: python frontend"], "type": "docs", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "### Next Steps\n\n1. **Update Documentation**: Add detailed explanations for the gradient behavior of `torch.min` and `torch.max`.\n2. **Implement Gradient Propagation**: Ensure that the gradient propagation logic aligns with the described behaviors.\n3. **Testing**: Perform comprehensive testing to verify the correctness of the updated gradient behavior.\n\n### Category\n\nDocumentation Improvement\n\n### Relevant References\n\n- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n\n### Additional Notes\n\n- The current implementation of `torch.min` and `torch.max` does not explicitly state their gradient behavior. This discrepancy needs to be addressed to ensure consistency and clarity in the documentation.\n- The proposed solution involves updating the documentation to include clear explanations of how these functions handle gradients, especially for multi-dimensional inputs and specific axes."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}]}
{"ts": "2025-08-10T22:31:49.751668+00:00", "repo": "pytorch/pytorch", "issue_number": 160272, "issue_url": "https://github.com/pytorch/pytorch/issues/160272", "title": "Add Green Context Support for fsdp2", "labels": ["oncall: distributed"], "type": "feature", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "### Feature Request/RFC\n\n**Scope & Rationale**\nThe proposed feature introduces CUDA Green Context support for `fsdp2`, which aims to improve the performance of PyTorch's data parallelism by providing isolated SM resources for computation and communication. This is particularly useful when dealing with high-throughput workloads where overlapping computation and communication can lead to degraded performance.\n\n**Minimal API or Design Sketch**\nTo implement CUDA Green Context support in `fsdp2`, we need to modify the existing codebase to include the necessary functionality. Here’s a simplified design sketch:\n\n1. **Context Management**: Create two contexts: one for computation (`computation_context`) and one for communication (`communication_context`).\n2. **Stream Creation**: For each operation, create a stream from the appropriate context.\n3. **Stream Assignment**: In `pre_forward` and `forward` hooks, assign the correct stream based on whether the operation involves computation or communication.\n4. **Back-compat Considerations**: Ensure that the new implementation does not break existing functionality and provides a smooth transition path.\n5. **Implementation Plan**: Implement the above changes in the relevant files and modules within `torch/distributed/fsdp`.\n6. **Test Strategy**: Write unit tests to verify that the new implementation works correctly and does not introduce any regressions. Additionally, write integration tests to ensure that the feature integrates seamlessly with other parts of the PyTorch ecosystem.\n\n**Short-term Workaround if Not Implemented**\nIf the feature is not implemented, a short-term workaround could be to disable CUDA Green Context support temporarily or to use a different approach to handle overlapping computation and communication.\n\n### Conclusion\nThis RFC outlines the scope and rationale for adding CUDA Green Context support to `fsdp2`. It provides a detailed design sketch, back-compat considerations, implementation plan, and test strategy. The goal is to ensure that the feature is implemented in a way that minimizes disruption to existing functionality and provides a seamless user experience."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}]}
{"ts": "2025-08-10T22:32:16.879568+00:00", "repo": "pytorch/pytorch", "issue_number": 160271, "issue_url": "https://github.com/pytorch/pytorch/issues/160271", "title": "Error when using torch.compile with dynamic=True: tensor 'guidance' stride mismatch at index 0", "labels": ["oncall: pt2"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "### Category: Performance Optimization\n\n### Next Steps:\n1. **Check Tensor Shapes**: Ensure that the shapes of tensors involved in the `forward` method match. The error message indicates that there's a mismatch in the shape of the `guidance` tensor at index 0.\n2. **Review Transformer Code**: Examine the `transformer` module in the `diffusers.models.transformers` directory for any potential issues related to tensor strides or dimensions.\n3. **Update Dependencies**: Ensure that all dependencies are up-to-date. Sometimes, bugs are fixed in newer versions of libraries.\n4. **Debugging**: Use `torch.autograd.set_detect_anomaly(True)` to enable anomaly detection during training, which can help identify where the issue lies.\n\n### Relevant References:\n- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n- [Dynamo Documentation](https://pytorch.org/docs/stable/dynamo.html)\n- [Diffusers Documentation](https://huggingface.co/docs/diffusers/en/main)\n\nBy following these steps, you should be able to resolve the `tensor 'guidance' stride mismatch at index 0` error and improve the performance of your model."}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}]}
{"ts": "2025-08-10T22:32:42.136550+00:00", "repo": "pytorch/pytorch", "issue_number": 160269, "issue_url": "https://github.com/pytorch/pytorch/issues/160269", "title": "Questions about Volta Support", "labels": ["module: docs", "oncall: releng", "triaged"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "**Category:** Documentation\n\n**Next Steps:**\n1. Review the documentation on the official PyTorch website or GitHub repository to ensure that the information is up-to-date.\n2. Check if there are any updates or changes in the release notes since the last update.\n3. If the documentation still does not mention Volta support, consider reaching out to the PyTorch community or maintainers for further clarification.\n\n**Relevant References:**\n- [PyTorch Release Notes](https://pytorch.org/docs/stable/releasenotes/index.html)\n- [Pull Request #157968](https://github.com/pytorch/pytorch/pull/157968)"}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}]}
{"ts": "2025-08-10T22:33:09.996784+00:00", "repo": "pytorch/pytorch", "issue_number": 160265, "issue_url": "https://github.com/pytorch/pytorch/issues/160265", "title": "XPU out of memory on Intel iGPUs although plenty of memory is available per error message.", "labels": ["triaged", "module: xpu"], "type": "other", "state": "open", "fixed": false, "solution_text": null, "rca_text": null, "models": [{"nickname": "deepseek-coder-1.3b", "family": "DeepSeek-Coder", "param_size": "1.3B", "hf_repo": "deepseek-ai/deepseek-coder-1.3b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "deepseek-coder-6.7b", "family": "DeepSeek-Coder", "param_size": "6.7B", "hf_repo": "deepseek-ai/deepseek-coder-6.7b-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "qwen2.5-coder-1.5b", "family": "Qwen2.5-Coder", "param_size": "1.5B", "hf_repo": "Qwen/Qwen2.5-Coder-1.5B-Instruct", "answer": "**Category:** Performance Issues\n\n**Next Steps:**\n1. **Check System Resources:** Ensure your system has sufficient VRAM (Virtual Graphics Memory) available for the GPU. You can check this by running `nvidia-smi` on Linux or `vraminfo` on macOS.\n2. **Update PyTorch and CUDA:** Make sure you are using the latest versions of PyTorch and CUDA. Sometimes, bugs are fixed in newer releases.\n3. **Use `empty_cache()`:** If the issue persists, try calling `model.empty_cache()` before running your model again. This will free up any unused memory.\n4. **Profile Your Code:** Use profiling tools like `torch.cuda.memory_profiler` to identify where memory is being allocated and released.\n5. **Report Issue:** If the problem persists, consider reporting the issue on the PyTorch GitHub repository along with detailed information about your hardware configuration and the steps you took to reproduce the issue.\n\n**Relevant References:**\n- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n- [NVIDIA GPU Information](https://developer.nvidia.com/gpu-info)\n- [CUDA Profiling Tools](https://docs.nvidia.com/cuda/profiler/)"}, {"nickname": "starcoder2-3b", "family": "StarCoder2", "param_size": "3B", "hf_repo": "bigcode/starcoder2-3b", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}, {"nickname": "phi-3-mini", "family": "Phi-3", "param_size": "3.8B", "hf_repo": "microsoft/Phi-3-mini-128k-instruct", "answer": "[MODEL ERROR] All attempts failed (text/chat). Check model task support or rate limits."}]}
